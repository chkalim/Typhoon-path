# -*- coding: utf-8 -*-
"""typhoon.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TH8ooY8N_qrzOlY11mrrf2JAlaVrF_RV
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt
from geopy.distance import great_circle

from sklearn import svm
from sklearn.metrics import accuracy_score
import sklearn.metrics

import torch
import torch.utils.data as Data
import datetime

import os
import random

# forecast 24-hour lead time
pre_seq = 4
batch_size = 128
epochs = 128

model_name1 = '/content/drive/My Drive/Colab Notebooks/typhoon/model_kfold/Model_STgru1.pkl'
model_name2 = '/content/drive/My Drive/Colab Notebooks/typhoon/model_kfold/Model_STgru2.pkl'
model_name3 = '/content/drive/My Drive/Colab Notebooks/typhoon/model_kfold/Model_STgru3.pkl'
model_name4 = '/content/drive/My Drive/Colab Notebooks/typhoon/model_kfold/Model_STgru4.pkl'
model_name5 = '/content/drive/My Drive/Colab Notebooks/typhoon/model_kfold/Model_STgru5.pkl'
model_name6 = '/content/drive/My Drive/Colab Notebooks/typhoon/model_kfold/Model_STgru6.pkl'
model_name7 = '/content/drive/My Drive/Colab Notebooks/typhoon/model_kfold/Model_STgru7.pkl'
model_name8 = '/content/drive/My Drive/Colab Notebooks/typhoon/model_kfold/Model_STgru8.pkl'
model_name9 = '/content/drive/My Drive/Colab Notebooks/typhoon/model_kfold/Model_STgru9.pkl'
model_name10 = '/content/drive/My Drive/Colab Notebooks/typhoon/model_kfold/Model_STgru10.pkl'
# model_name = 'Model.pkl'
os.environ["CUDA_VISIBLE_DEVICES"] = "1"

import torch
import torch.nn as nn

from torch.nn.parameter import Parameter
from torch.autograd import Variable
import torch.nn.functional as F

import math
train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/typhoon/data/CMA_train_'+str(pre_seq*6)+'h.csv', header=None)
test= pd.read_csv('/content/drive/My Drive/Colab Notebooks/typhoon/data/CMA_test_'+str(pre_seq*6)+'h.csv', header=None)

# train = pd.read_csv('data/CMA_train_'+str(pre_seq*6)+'h.csv', header=None)
# test= pd.read_csv('data/CMA_test_'+str(pre_seq*6)+'h.csv', header=None)

train.shape, test.shape



class TrainLoader(Data.Dataset):
    def __init__(self, X_wide_train, y_train):
        self.X_wide_train = X_wide_train
        self.y_train = y_train

    def __getitem__(self, index):
        return [self.X_wide_train[index]], self.y_train[index]

    def __len__(self):
        return len(self.X_wide_train)


CLIPER_feature =  pd.concat((train, test), axis=0)
CLIPER_feature.reset_index(drop=True, inplace=True)

X_wide_scaler = MinMaxScaler()
y_scaler = MinMaxScaler()

X_wide = X_wide_scaler.fit_transform(CLIPER_feature.iloc[:, 6:])
X_wide_train = X_wide[0: train.shape[0], :]

y = y_scaler.fit_transform(CLIPER_feature.loc[:, 3:4])


#****** modified


y_train = y[0: train.shape[0], :]
#********** here is the code for extracting test code
X_wide_test = X_wide[train.shape[0]: , :]
y_test = y[train.shape[0]: , :]

#************ code to insiate the model

class GRU(nn.Module):

    def __init__(self, num_classes, input_size, hidden_size, num_layers):
        super(GRU, self).__init__()

        self.num_classes = num_classes
        self.num_layers = num_layers
        self.input_size = input_size
        self.hidden_size = hidden_size

        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size,
                            num_layers=num_layers, batch_first=True)

        self.gru2 = nn.GRU(input_size=hidden_size, hidden_size=hidden_size,
                           num_layers=num_layers, batch_first=True)

        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x, test=0):
        x = x.unsqueeze(1)
        h_0 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_size))
        h_1 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_size))

        ula,_  = self.gru(x, h_0)

        # h_out = ula.view(-1, self.hidden_size)
        #
        # h_out = h_out.unsqueeze(1)
        ula2, _ = self.gru2(ula,h_1)

        h_out = ula2.view(-1, self.hidden_size)
        out = self.fc(h_out)

        return out


class TrainLoader(Data.Dataset):
    def __init__(self, X_wide_train, y_train):
        self.X_wide_train = X_wide_train
        self.y_train = y_train

    def __getitem__(self, index):
        return self.X_wide_train[index], self.y_train[index]

    def __len__(self):
        return len(self.X_wide_train)



limit=int((len(X_wide_train)/100)*50)
X_wide_train = X_wide[0: limit, :]
y_train = y[0: limit, :]

print(len(X_wide_train))
print(len(y_train))


learning_rate = 0.01

input_size = 53
hidden_size = 128
num_layers = 1

num_classes = 2

model1 = GRU(num_classes, input_size, hidden_size, num_layers)
model2 = GRU(num_classes, input_size, hidden_size, num_layers)
model3 = GRU(num_classes, input_size, hidden_size, num_layers)
model4 = GRU(num_classes, input_size, hidden_size, num_layers)
model5 = GRU(num_classes, input_size, hidden_size, num_layers)
model6 = GRU(num_classes, input_size, hidden_size, num_layers)
model7 = GRU(num_classes, input_size, hidden_size, num_layers)
model8 = GRU(num_classes, input_size, hidden_size, num_layers)
model9 = GRU(num_classes, input_size, hidden_size, num_layers)
model10 = GRU(num_classes, input_size, hidden_size, num_layers)


if torch.cuda.is_available():
    model.cuda()
"""# Training"""

criterion = nn.L1Loss()
optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.001)
optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.001)
optimizer3 = torch.optim.Adam(model3.parameters(), lr=0.001)
optimizer4 = torch.optim.Adam(model4.parameters(), lr=0.001)
optimizer5 = torch.optim.Adam(model5.parameters(), lr=0.001)
optimizer6 = torch.optim.Adam(model6.parameters(), lr=0.001)
optimizer7 = torch.optim.Adam(model7.parameters(), lr=0.001)
optimizer8 = torch.optim.Adam(model8.parameters(), lr=0.001)
optimizer9 = torch.optim.Adam(model9.parameters(), lr=0.001)
optimizer10 = torch.optim.Adam(model10.parameters(), lr=0.001)

full_train_index = [*range(0, len(X_wide_train))]
#train_index, val_index, _, _, = train_test_split(full_train_index, full_train_index, test_size=0.1)
from sklearn.model_selection import KFold
k_folds=10

kfold = KFold(n_splits=k_folds, shuffle=True)
index_count=1
for fold, (train_ids, val_ids) in enumerate(kfold.split(X_wide_train)):
    min_val_loss = 100
    if index_count ==1:
        model=model1
        optimizer=optimizer1
        model_name=model_name1
    elif index_count ==2:
        model=model2
        optimizer=optimizer2
        model_name=model_name2
    elif index_count ==3:
        model=model3
        optimizer=optimizer3
        model_name=model_name3
    elif index_count ==4:
        model=model4
        optimizer=optimizer4
        model_name=model_name4
    elif index_count ==5:
        model=model5
        optimizer=optimizer5
        model_name=model_name5
    elif index_count ==6:
        model=model6
        optimizer=optimizer6
        model_name=model_name6
    elif index_count ==7:
        model=model7
        optimizer=optimizer7
        model_name=model_name7
    elif index_count ==8:
        model=model8
        optimizer=optimizer8
        model_name=model_name8
    elif index_count ==9:
        model=model9
        optimizer=optimizer9
        model_name=model_name9
    else:
        model=model10
        optimizer=optimizer10
        model_name=model_name10
            
    print("training starts for fold",index_count)
    for epoch in range(epochs):  # loop over the dataset multiple times
        starttime = datetime.datetime.now()

        train_dataset = torch.utils.data.DataLoader(
            TrainLoader(X_wide_train[train_ids], y_train[train_ids]),
            batch_size=batch_size, shuffle=True)

        val_dataset = torch.utils.data.DataLoader(
            TrainLoader(X_wide_train[val_ids], y_train[val_ids]),
            batch_size=batch_size, shuffle=True)

        # training
        total_train_loss = 0
        for step, (batch_x, batch_y) in enumerate(train_dataset):
            X_wide_train_cuda = batch_x.float()

            y_train_cuda = batch_y
            # zero the parameter gradients
            optimizer.zero_grad()
            pred_y = model(X_wide_train_cuda)
            loss = criterion(pred_y, y_train_cuda)
            total_train_loss += loss.item()
            loss.backward()
            optimizer.step()
            # forward + backward + optimize                

        # validation
        total_val_loss = 0
        for _, (batch_val_x, batch_val_y) in enumerate(val_dataset):
            X_wide_val_cuda = batch_val_x.float()
            y_val_cuda = batch_val_y

            pred_y = model(X_wide_val_cuda)
            val_loss = criterion(pred_y, y_val_cuda)
            total_val_loss += val_loss.item()

            # print statistics
        if min_val_loss > total_val_loss:
            torch.save(model.state_dict(), model_name)
            min_val_loss = total_val_loss
        endtime = datetime.datetime.now()
        print('epochs [%d/%d] cost:%.2fs train_loss: %.5f val_loss: %.5f' %
              (epoch + 1, epochs, (endtime - starttime).seconds, total_train_loss, total_val_loss))
    index_count=index_count+1
print('Finished Training')

# here is the code for the testing

# net.load_state_dict(torch.load(model_name))
years = test[5].unique()
test_list = []

for year in years:
    temp = test[test[5] == year]
    temp = temp.reset_index(drop=True)
    test_list.append(temp)

len(test_list)

torch.cuda.empty_cache()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# net = model
# net = net.to(device)
model1.load_state_dict(torch.load(model_name1))
model2.load_state_dict(torch.load(model_name2))
model3.load_state_dict(torch.load(model_name3))
model4.load_state_dict(torch.load(model_name4))
model5.load_state_dict(torch.load(model_name5))
model6.load_state_dict(torch.load(model_name6))
model7.load_state_dict(torch.load(model_name7))
model8.load_state_dict(torch.load(model_name8))
model9.load_state_dict(torch.load(model_name9))
model10.load_state_dict(torch.load(model_name10))


# model.eval()

with torch.no_grad():
    for year, _test in zip(years, test_list):

        print(year, 'Ã¥Â¹Â´:')

        y_test_lat = _test.loc[:, 3]

        y_test_long = _test.loc[:, 4]

        X_wide_test = X_wide_scaler.transform(_test.loc[:, 6:])
        # X_wide_test = X_wide_scaler.fit|_transform(_test.loc[:, 6:])
        # X_wide_test = _test.loc[:, 6:]

        final_test_list = []

        # if torch.cuda.is_available():
        X_wide_test = Variable(torch.from_numpy(X_wide_test).float())
        # X_wide_test = X_wide_test.float().cuda()
        pred1 = model1(X_wide_test)
        pred2 = model2(X_wide_test)
        pred3 = model3(X_wide_test)
        pred4 = model4(X_wide_test)
        pred5 = model5(X_wide_test)
        pred6 = model6(X_wide_test)
        pred7 = model7(X_wide_test)
        pred8 = model8(X_wide_test)
        pred9 = model9(X_wide_test)
        pred10 = model10(X_wide_test)
        
        result = torch.sum(torch.stack([pred1,pred2,pred3,pred4,pred5,pred6,pred7,pred8,pred9,pred10]), dim=0)
        result2=torch.div(result,10)
        pred = y_scaler.inverse_transform(result2.cpu().detach().numpy())

        pred_lat = pred[:, 0]
        pred_long = pred[:, 1]
        true_lat = y_test_lat
        true_long = y_test_long

        diff_lat = np.abs(pred_lat - true_lat)
        diff_long = np.abs(pred_long - true_long)

        print('avg lat:', sum(diff_lat) / len(diff_lat))
        print('avg long:', sum(diff_long) / len(diff_long))

        sum_error = []
        for i in range(0, len(pred_lat)):
            sum_error.append(great_circle((pred_lat[i], pred_long[i]), (true_lat[i], true_long[i])).kilometers)

        print('avg distance error:', sum(sum_error) / len(sum_error))

