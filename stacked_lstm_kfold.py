# -*- coding: utf-8 -*-
"""typhoon.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TH8ooY8N_qrzOlY11mrrf2JAlaVrF_RV
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt
from geopy.distance import great_circle

from sklearn import svm
from sklearn.metrics import accuracy_score
import sklearn.metrics

import torch
import torch.utils.data as Data
import datetime

import os
import random

# forecast 24-hour lead time
pre_seq = 4
batch_size = 128
epochs = 128

model_name1 = '/content/drive/My Drive/Colab Notebooks/typhoon/model_kfold/Model_STlstm1.pkl'
model_name2 = '/content/drive/My Drive/Colab Notebooks/typhoon/model_kfold/Model_STlstm2.pkl'
model_name3 = '/content/drive/My Drive/Colab Notebooks/typhoon/model_kfold/Model_STlstm3.pkl'
model_name4 = '/content/drive/My Drive/Colab Notebooks/typhoon/model_kfold/Model_STlstm4.pkl'
model_name5 = '/content/drive/My Drive/Colab Notebooks/typhoon/model_kfold/Model_STlstm5.pkl'
model_name6 = '/content/drive/My Drive/Colab Notebooks/typhoon/model_kfold/Model_STlstm6.pkl'
model_name7 = '/content/drive/My Drive/Colab Notebooks/typhoon/model_kfold/Model_STlstm7.pkl'
model_name8 = '/content/drive/My Drive/Colab Notebooks/typhoon/model_kfold/Model_STlstm8.pkl'
model_name9 = '/content/drive/My Drive/Colab Notebooks/typhoon/model_kfold/Model_STlstm9.pkl'
model_name10 = '/content/drive/My Drive/Colab Notebooks/typhoon/model_kfold/Model_STlstm10.pkl'
# model_name = 'Model.pkl'
os.environ["CUDA_VISIBLE_DEVICES"] = "1"

import torch
import torch.nn as nn

from torch.nn.parameter import Parameter
from torch.autograd import Variable
import torch.nn.functional as F

import math
train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/typhoon/data/CMA_train_'+str(pre_seq*6)+'h.csv', header=None)
test= pd.read_csv('/content/drive/My Drive/Colab Notebooks/typhoon/data/CMA_test_'+str(pre_seq*6)+'h.csv', header=None)

# train = pd.read_csv('data/CMA_train_'+str(pre_seq*6)+'h.csv', header=None)
# test= pd.read_csv('data/CMA_test_'+str(pre_seq*6)+'h.csv', header=None)

train.shape, test.shape



class TrainLoader(Data.Dataset):
    def __init__(self, X_wide_train, y_train):
        self.X_wide_train = X_wide_train
        self.y_train = y_train

    def __getitem__(self, index):
        return [self.X_wide_train[index]], self.y_train[index]

    def __len__(self):
        return len(self.X_wide_train)


CLIPER_feature =  pd.concat((train, test), axis=0)
CLIPER_feature.reset_index(drop=True, inplace=True)

X_wide_scaler = MinMaxScaler()
y_scaler = MinMaxScaler()

X_wide = X_wide_scaler.fit_transform(CLIPER_feature.iloc[:, 6:])
X_wide_train = X_wide[0: train.shape[0], :]

y = y_scaler.fit_transform(CLIPER_feature.loc[:, 3:4])


#****** modified


y_train = y[0: train.shape[0], :]

print("************************************************")
print(len(X_wide_train))
print(len(y_train))

#********** here is the code for extracting test code
X_wide_test = X_wide[train.shape[0]: , :]
y_test = y[train.shape[0]: , :]

print(len(X_wide_test))
print(len(y_test))
print(len(X_wide_train))

#************ code to insiate the model

class LSTM(nn.Module):

    def __init__(self, num_classes, input_size, hidden_size, num_layers):
        super(LSTM, self).__init__()

        self.num_classes = num_classes
        self.num_layers = num_layers
        self.input_size = input_size
        self.hidden_size = hidden_size

        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,
                            num_layers=num_layers, batch_first=True)
        self.lstm2 = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size,
                            num_layers=num_layers, batch_first=True)
        self.lstm3 = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size,
                            num_layers=num_layers, batch_first=True)
        
        self.lstm4 = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size,
                            num_layers=num_layers, batch_first=True)

        self.fc1 = nn.Linear(hidden_size, 53)

        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x, test=0):
        x = x.unsqueeze(1)
        h_0 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_size))

        c_0 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_size))

        h_1 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_size))

        c_1 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_size))
        
        h_2 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_size))

        c_2 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_size))
        
        h_3 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_size))

        c_3 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_size))
        
       

        ula, (h_out, _) = self.lstm(x, (h_0, c_0))
        h_out = h_out.view(-1, self.hidden_size)
        # out = self.fc1(h_out)
        out = h_out.unsqueeze(1)

        ula2, (h_out2, _) = self.lstm2(out, (h_1, c_1))
        h_out2 = h_out2.view(-1, self.hidden_size)
        
        out2 = h_out2.unsqueeze(1)
        ula3, (h_out3, _) = self.lstm3(out2, (h_2, c_2))
        h_out3 = h_out3.view(-1, self.hidden_size)
        # out3 = self.fc(h_out3)
        
        out3 = h_out3.unsqueeze(1)
        ula4, (h_out4, _) = self.lstm4(out3, (h_3, c_3))
        h_out4 = h_out4.view(-1, self.hidden_size)
        out4 = self.fc(h_out4)
        
        
        # out4 = h_out4.unsqueeze(1)
        # ula5, (h_out5, _) = self.lstm5(out4, (h_4, c_4))
        # h_out5 = h_out5.view(-1, self.hidden_size)
        # out5 = self.fc(h_out5)
        
        # out5 = h_out5.unsqueeze(1)
        # ula5, (h_out6, _) = self.lstm6(out5, (h_5, c_5))
        # h_out6 = h_out6.view(-1, self.hidden_size)
        # out6 = self.fc(h_out6)
        
        # out6 = h_out6.unsqueeze(1)
        # ula7, (h_out7, _) = self.lstm7(out6, (h_6, c_6))
        # h_out7 = h_out7.view(-1, self.hidden_size)
        # out6 = self.fc(h_out7)
        
        # out7 = h_out6.unsqueeze(1)
        # ula8, (h_out8, _) = self.lstm8(out7, (h_7, c_7))
        # h_out8 = h_out8.view(-1, self.hidden_size)
        # out8 = self.fc(h_out8)
        
        # out8 = h_out8.unsqueeze(1)
        # ula9, (h_out9, _) = self.lstm9(out8, (h_8, c_8))
        # h_out9 = h_out9.view(-1, self.hidden_size)
        # out9 = self.fc(h_out9)
        
        return out4


class TrainLoader(Data.Dataset):
    def __init__(self, X_wide_train, y_train):
        self.X_wide_train = X_wide_train
        self.y_train = y_train

    def __getitem__(self, index):
        return self.X_wide_train[index], self.y_train[index]

    def __len__(self):
        return len(self.X_wide_train)



# limit=int((len(X_wide_train)/100)*50)
# X_wide_train = X_wide[0: limit, :]
# y_train = y[0: limit, :]
# print(len(X_wide_train))
# print(len(y_train))


learning_rate = 0.01

input_size = 53
hidden_size = 128
num_layers = 1

num_classes = 2

model1 = LSTM(num_classes, input_size, hidden_size, num_layers)
model2 = LSTM(num_classes, input_size, hidden_size, num_layers)
model3 = LSTM(num_classes, input_size, hidden_size, num_layers)
model4 = LSTM(num_classes, input_size, hidden_size, num_layers)
model5 = LSTM(num_classes, input_size, hidden_size, num_layers)
model6 = LSTM(num_classes, input_size, hidden_size, num_layers)
model7 = LSTM(num_classes, input_size, hidden_size, num_layers)
model8 = LSTM(num_classes, input_size, hidden_size, num_layers)
model9 = LSTM(num_classes, input_size, hidden_size, num_layers)
model10 = LSTM(num_classes, input_size, hidden_size, num_layers)


if torch.cuda.is_available():
    model.cuda()
"""# Training"""

criterion = nn.L1Loss()
optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.001)
optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.001)
optimizer3 = torch.optim.Adam(model3.parameters(), lr=0.001)
optimizer4 = torch.optim.Adam(model4.parameters(), lr=0.001)
optimizer5 = torch.optim.Adam(model5.parameters(), lr=0.001)
optimizer6 = torch.optim.Adam(model6.parameters(), lr=0.001)
optimizer7 = torch.optim.Adam(model7.parameters(), lr=0.001)
optimizer8 = torch.optim.Adam(model8.parameters(), lr=0.001)
optimizer9 = torch.optim.Adam(model9.parameters(), lr=0.001)
optimizer10 = torch.optim.Adam(model10.parameters(), lr=0.001)

full_train_index = [*range(0, len(X_wide_train))]
#train_index, val_index, _, _, = train_test_split(full_train_index, full_train_index, test_size=0.1)
from sklearn.model_selection import KFold
k_folds=10

kfold = KFold(n_splits=k_folds, shuffle=True)
index_count=1
for fold, (train_ids, val_ids) in enumerate(kfold.split(X_wide_train)):
    min_val_loss = 100
    if index_count ==1:
        model=model1
        optimizer=optimizer1
        model_name=model_name1
    elif index_count ==2:
        model=model2
        optimizer=optimizer2
        model_name=model_name2
    elif index_count ==3:
        model=model3
        optimizer=optimizer3
        model_name=model_name3
    elif index_count ==4:
        model=model4
        optimizer=optimizer4
        model_name=model_name4
    elif index_count ==5:
        model=model5
        optimizer=optimizer5
        model_name=model_name5
    elif index_count ==6:
        model=model6
        optimizer=optimizer6
        model_name=model_name6
    elif index_count ==7:
        model=model7
        optimizer=optimizer7
        model_name=model_name7
    elif index_count ==8:
        model=model8
        optimizer=optimizer8
        model_name=model_name8
    elif index_count ==9:
        model=model9
        optimizer=optimizer9
        model_name=model_name9
    else:
        model=model10
        optimizer=optimizer10
        model_name=model_name10
            
    print("training starts for fold",index_count)
    for epoch in range(epochs):  # loop over the dataset multiple times
        starttime = datetime.datetime.now()

        train_dataset = torch.utils.data.DataLoader(
            TrainLoader(X_wide_train[train_ids], y_train[train_ids]),
            batch_size=batch_size, shuffle=True)

        val_dataset = torch.utils.data.DataLoader(
            TrainLoader(X_wide_train[val_ids], y_train[val_ids]),
            batch_size=batch_size, shuffle=True)

        # training
        total_train_loss = 0
        for step, (batch_x, batch_y) in enumerate(train_dataset):
            X_wide_train_cuda = batch_x.float()

            y_train_cuda = batch_y
            # zero the parameter gradients
            optimizer.zero_grad()
            pred_y = model(X_wide_train_cuda)
            loss = criterion(pred_y, y_train_cuda)
            total_train_loss += loss.item()
            loss.backward()
            optimizer.step()
            # forward + backward + optimize                

        # validation
        total_val_loss = 0
        for _, (batch_val_x, batch_val_y) in enumerate(val_dataset):
            X_wide_val_cuda = batch_val_x.float()
            y_val_cuda = batch_val_y

            pred_y = model(X_wide_val_cuda)
            val_loss = criterion(pred_y, y_val_cuda)
            total_val_loss += val_loss.item()

            # print statistics
        if min_val_loss > total_val_loss:
            torch.save(model.state_dict(), model_name)
            min_val_loss = total_val_loss
        endtime = datetime.datetime.now()
        print('epochs [%d/%d] cost:%.2fs train_loss: %.5f val_loss: %.5f' %
              (epoch + 1, epochs, (endtime - starttime).seconds, total_train_loss, total_val_loss))
    index_count=index_count+1
print('Finished Training')

# here is the code for the testing

# net.load_state_dict(torch.load(model_name))
years = test[5].unique()
test_list = []

for year in years:
    temp = test[test[5] == year]
    temp = temp.reset_index(drop=True)
    test_list.append(temp)

len(test_list)

torch.cuda.empty_cache()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# net = model
# net = net.to(device)
model1.load_state_dict(torch.load(model_name1))
model2.load_state_dict(torch.load(model_name2))
model3.load_state_dict(torch.load(model_name3))
model4.load_state_dict(torch.load(model_name4))
model5.load_state_dict(torch.load(model_name5))
model6.load_state_dict(torch.load(model_name6))
model7.load_state_dict(torch.load(model_name7))
model8.load_state_dict(torch.load(model_name8))
model9.load_state_dict(torch.load(model_name9))
model10.load_state_dict(torch.load(model_name10))


# model.eval()

with torch.no_grad():
    for year, _test in zip(years, test_list):

        print(year, 'Ã¥Â¹Â´:')

        y_test_lat = _test.loc[:, 3]

        y_test_long = _test.loc[:, 4]

        X_wide_test = X_wide_scaler.transform(_test.loc[:, 6:])
        # X_wide_test = X_wide_scaler.fit|_transform(_test.loc[:, 6:])
        # X_wide_test = _test.loc[:, 6:]

        final_test_list = []

        # if torch.cuda.is_available():
        X_wide_test = Variable(torch.from_numpy(X_wide_test).float())
        # X_wide_test = X_wide_test.float().cuda()
        pred1 = model1(X_wide_test)
        pred2 = model2(X_wide_test)
        pred3 = model3(X_wide_test)
        pred4 = model4(X_wide_test)
        pred5 = model5(X_wide_test)
        pred6 = model6(X_wide_test)
        pred7 = model7(X_wide_test)
        pred8 = model8(X_wide_test)
        pred9 = model9(X_wide_test)
        pred10 = model10(X_wide_test)
        
        result = torch.sum(torch.stack([pred1,pred2,pred3,pred4,pred5,pred6,pred7,pred8,pred9,pred10]), dim=0)
        result2=torch.div(result,10)
        pred = y_scaler.inverse_transform(result2.cpu().detach().numpy())

        pred_lat = pred[:, 0]
        pred_long = pred[:, 1]
        true_lat = y_test_lat
        true_long = y_test_long

        diff_lat = np.abs(pred_lat - true_lat)
        diff_long = np.abs(pred_long - true_long)

        print('avg lat:', sum(diff_lat) / len(diff_lat))
        print('avg long:', sum(diff_long) / len(diff_long))

        sum_error = []
        for i in range(0, len(pred_lat)):
            sum_error.append(great_circle((pred_lat[i], pred_long[i]), (true_lat[i], true_long[i])).kilometers)

        print('avg distance error:', sum(sum_error) / len(sum_error))

